"""
run_pipeline.py — End-to-end EcoSurve pipeline entry point.

Pipeline stages
---------------
1. **Captioning**   — BLIP samples frames from each video and generates
                      natural-language captions.
2. **Summarisation** — BART compresses per-segment captions into one sentence
                       per 5-second window, then produces a single incident-level
                       summary for the whole video.
3. **Indexing**     — Summaries are embedded with SentenceTransformers and
                       stored in a persistent ChromaDB vector store.
4. **Chat** (opt.)  — Interactive RAG Q&A powered by GPT-4o-mini.

Usage examples
--------------
# Full run (caption → summarise → index):
    python -m src.pipeline.run_pipeline

# Skip re-captioning if summaries already exist, then chat:
    python -m src.pipeline.run_pipeline --skip_summaries --chat

# Custom models:
    python -m src.pipeline.run_pipeline --blip_model Salesforce/blip2-opt-2.7b
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

from dotenv import load_dotenv
from tqdm import tqdm

# Load OPENAI_API_KEY (and any other .env vars) before importing submodules.
load_dotenv()

from src.config import Paths, PipelineConfig
from src.rag.chat import interactive_chat
from src.rag.index import build_chroma_from_summaries
from src.summarization.segment_summarizer import SegmentSummarizer
from src.video.frames import iter_sampled_frames, read_video_meta
from src.video.io import list_videos
from src.vlm.blip_captioner import BlipCaptioner


# ---------------------------------------------------------------------------
# Core processing
# ---------------------------------------------------------------------------

def _video_id(path: Path) -> str:
    """Derive a stable, filesystem-safe identifier from a video's filename."""
    return path.stem


def generate_summaries_for_video(
    video_path: Path,
    out_dir: Path,
    cfg: PipelineConfig,
    captioner: BlipCaptioner,
    summarizer: SegmentSummarizer,
) -> dict:
    """
    Run the caption → summarise pipeline for a single video file.

    The video is processed in non-overlapping segments of `cfg.segment_seconds`
    duration. For each segment, frames are sampled at `cfg.sample_fps`, captioned
    by BLIP, and then compressed into a single sentence by BART. After all
    segments are processed, a final incident-level summary is generated by
    passing all segment sentences back through the summariser.

    Output files written to *out_dir*:
    - ``<video_id>.json`` — machine-readable payload (used by the indexer).
    - ``<video_id>.txt``  — human-readable summary (useful for quick inspection).

    Parameters
    ----------
    video_path:
        Path to the video file to process.
    out_dir:
        Directory where output JSON and TXT files will be written.
    cfg:
        Pipeline configuration (sample rate, segment length, model names, etc.).
    captioner:
        Pre-loaded BlipCaptioner instance (shared across videos to avoid reloads).
    summarizer:
        Pre-loaded SegmentSummarizer instance (shared across videos).

    Returns
    -------
    dict
        A dict with keys ``video_id``, ``json``, and ``txt`` pointing to the
        generated output files.
    """
    out_dir.mkdir(parents=True, exist_ok=True)

    meta = read_video_meta(str(video_path))
    video_id = _video_id(video_path)

    seg_len = float(cfg.segment_seconds)
    duration = float(meta.duration_s)

    timestamps: list[list[float]] = []
    sentences: list[str] = []

    t0 = 0.0
    total_segments = max(1, int(duration // seg_len) + 1)

    with tqdm(total=total_segments, desc=f"Summarizing {video_id}", unit="seg") as pbar:
        while t0 < duration:
            t1 = min(duration, t0 + seg_len)

            # --- 1. Sample frames from the current segment window ---
            frames = list(
                iter_sampled_frames(
                    video_path=str(video_path),
                    sample_fps=cfg.sample_fps,
                    start_s=t0,
                    end_s=t1,
                    max_frames=cfg.max_frames_per_segment,
                )
            )

            # --- 2. Caption each frame with BLIP ---
            captions: list[str] = []
            for _ts, frame in frames:
                cap = captioner.caption_bgr(frame)
                if isinstance(cap, str) and cap.strip():
                    captions.append(cap.strip())

            # --- 3. Summarise captions into one segment sentence ---
            if not captions:
                seg_summary = "No clear visual content detected in this segment."
            else:
                seg_summary = summarizer.summarize_captions(captions)
                if not isinstance(seg_summary, str) or not seg_summary.strip():
                    seg_summary = "Summary unavailable for this segment."

            timestamps.append([round(t0, 2), round(t1, 2)])
            sentences.append(seg_summary.strip())

            t0 = t1
            pbar.update(1)

    # --- 4. Produce a single incident-level summary over the whole video ---
    # Filter out placeholder sentences so they don't pollute the top-level summary.
    meaningful = [
        s for s in sentences
        if isinstance(s, str)
        and s.strip()
        and "unavailable" not in s.lower()
        and "no clear visual" not in s.lower()
    ]

    if meaningful:
        # Pass the list of segment sentences directly (not wrapped in a single
        # string) so the chunking logic inside summarize_captions() can work properly.
        incident_summary = summarizer.summarize_captions(meaningful)
        if not isinstance(incident_summary, str) or not incident_summary.strip():
            incident_summary = "Incident summary unavailable."
        else:
            incident_summary = incident_summary.strip()
    else:
        incident_summary = "Incident summary unavailable."

    # --- 5. Persist outputs ---
    payload = {
        video_id: {
            "duration": int(round(duration)),
            "timestamps": timestamps,
            "sentences": sentences,
            "incident_summary": incident_summary,
        }
    }

    json_path = out_dir / f"{video_id}.json"
    txt_path = out_dir / f"{video_id}.txt"

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)

    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(f"VIDEO: {video_id}\n")
        f.write(f"DURATION: {int(round(duration))}s\n\n")
        f.write("INCIDENT SUMMARY:\n")
        f.write(f"{incident_summary}\n\n")
        f.write("SEGMENT SUMMARIES:\n")
        for (start, end), sent in zip(timestamps, sentences):
            f.write(f"[{start:.2f}–{end:.2f}s] {sent}\n")

    return {"video_id": video_id, "json": str(json_path), "txt": str(txt_path)}


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

def _build_arg_parser(cfg: PipelineConfig, paths: Paths) -> argparse.ArgumentParser:
    """Construct the argument parser with defaults pulled from config objects."""
    p = argparse.ArgumentParser(
        description="EcoSurve: VLM-powered crime footage summarisation + RAG Q&A",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Filesystem overrides
    p.add_argument("--videos_dir", default=str(paths.videos_dir))
    p.add_argument("--summaries_dir", default=str(paths.summaries_dir))
    p.add_argument("--chroma_dir", default=str(paths.chroma_dir))

    # Pipeline knobs
    p.add_argument("--sample_fps", type=float, default=cfg.sample_fps)
    p.add_argument("--segment_seconds", type=float, default=cfg.segment_seconds)
    p.add_argument("--max_frames_per_segment", type=int, default=cfg.max_frames_per_segment)
    p.add_argument("--blip_model", default=cfg.blip_model)
    p.add_argument("--summarizer_model", default=cfg.summarizer_model)

    # RAG knobs
    p.add_argument("--embedder_model", default=cfg.embedder_model)
    p.add_argument("--top_k", type=int, default=cfg.top_k)

    # Stage toggles
    p.add_argument("--skip_summaries", action="store_true",
                   help="Skip captioning/summarisation (use existing JSON files).")
    p.add_argument("--skip_index", action="store_true",
                   help="Skip rebuilding the Chroma vector index.")
    p.add_argument("--chat", action="store_true",
                   help="Launch interactive RAG Q&A after indexing.")

    return p


def main() -> None:
    paths = Paths()
    cfg = PipelineConfig()

    args = _build_arg_parser(cfg, paths).parse_args()

    # Reconstruct config from CLI args (allows per-run overrides without editing files).
    cfg = PipelineConfig(
        sample_fps=args.sample_fps,
        segment_seconds=args.segment_seconds,
        max_frames_per_segment=args.max_frames_per_segment,
        blip_model=args.blip_model,
        summarizer_model=args.summarizer_model,
        embedder_model=args.embedder_model,
        top_k=args.top_k,
    )

    videos_dir = Path(args.videos_dir)
    summaries_dir = Path(args.summaries_dir)
    chroma_dir = Path(args.chroma_dir)
    summaries_dir.mkdir(parents=True, exist_ok=True)

    # --- Stage 1 & 2: Captioning + Summarisation ---
    if not args.skip_summaries:
        videos = list_videos(videos_dir)
        if not videos:
            raise SystemExit(f"[EcoSurve] No videos found in: {videos_dir}")

        # Load models ONCE and share across all videos to avoid redundant downloads.
        print(f"\n[EcoSurve] Loading captioner  : {cfg.blip_model}")
        captioner = BlipCaptioner(cfg.blip_model)
        print(f"[EcoSurve] Loading summarizer : {cfg.summarizer_model}\n")
        summarizer = SegmentSummarizer(cfg.summarizer_model)

        results = [
            generate_summaries_for_video(vp, summaries_dir, cfg, captioner, summarizer)
            for vp in videos
        ]
        print("\n[EcoSurve] Generated summaries:")
        for r in results:
            print(f"  ✓ {r['video_id']}  →  {r['txt']}")
    else:
        print("[EcoSurve] Skipping summary generation.")

    # --- Stage 3: Indexing ---
    if not args.skip_index:
        info = build_chroma_from_summaries(
            summaries_dir=summaries_dir,
            chroma_dir=chroma_dir,
            embedder_name=cfg.embedder_model,
        )
        print(f"\n[EcoSurve] Chroma index built: {info['count']} documents → {info['chroma_dir']}")
    else:
        print("[EcoSurve] Skipping index build.")

    # --- Stage 4: Interactive chat (optional) ---
    if args.chat:
        interactive_chat(
            chroma_dir=chroma_dir,
            embedder_name=cfg.embedder_model,
            top_k=cfg.top_k,
        )


if __name__ == "__main__":
    main()